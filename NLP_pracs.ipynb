{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LejN3PdeQg4k",
        "outputId": "babd39bc-4ad4-4b38-cbcc-5403bcc33d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"all\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwJprn6fQoAS",
        "outputId": "e1f10bbe-4a73-4e8c-9328-7e924eaffd0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PORTER STEMMER\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "print(porter.stem(\"play\"))\n",
        "print(porter.stem(\"playing\"))\n",
        "print(porter.stem(\"plays\"))\n",
        "print(porter.stem(\"played\"))\n",
        "\n",
        "sentence = \"Programmers program with programming languages\"\n",
        "s2 = \"my dog is very playful\"\n",
        "words = word_tokenize(sentence +\" \"+ s2)\n",
        "\n",
        "for w in words:\n",
        "  print(w, \" : \", porter.stem(w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvdCM5SLQrxk",
        "outputId": "b0101697-5702-4f5a-e20c-8968c4a7d103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "play\n",
            "play\n",
            "play\n",
            "play\n",
            "Programmers  :  programm\n",
            "program  :  program\n",
            "with  :  with\n",
            "programming  :  program\n",
            "languages  :  languag\n",
            "my  :  my\n",
            "dog  :  dog\n",
            "is  :  is\n",
            "very  :  veri\n",
            "playful  :  play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Snowball stemming algorithm\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "snow = SnowballStemmer(language='english')\n",
        "sentence = \"Programmers coded with programming languages and using different framework and technologies\"\n",
        "\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "for w in words:\n",
        "  print(w, \" : \", snow.stem(w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivEKcaA3QwCy",
        "outputId": "63190d4d-f41a-4f1b-8254-36af504eb80b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Programmers  :  programm\n",
            "coded  :  code\n",
            "with  :  with\n",
            "programming  :  program\n",
            "languages  :  languag\n",
            "and  :  and\n",
            "using  :  use\n",
            "different  :  differ\n",
            "framework  :  framework\n",
            "and  :  and\n",
            "technologies  :  technolog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lancaster Stemmer\n",
        "\n",
        "from nltk.stem import LancasterStemmer\n",
        "Lanc_stemmer = LancasterStemmer()\n",
        "sentence = \"Programmers program with programming languages\"\n",
        "\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "for w in words:\n",
        "  print(w, \" : \", Lanc_stemmer.stem(w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY2HMHW_ReNk",
        "outputId": "8573ba9b-f5f7-4e95-cd0c-2adbd2369911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Programmers  :  program\n",
            "program  :  program\n",
            "with  :  with\n",
            "programming  :  program\n",
            "languages  :  langu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REGEX\n",
        "from nltk.stem import RegexpStemmer\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$|ion$', min=4)\n",
        "words = ['connecting','connect','connects','fractionally','fractions',\"consult\",\"consulation\", \"consulting\", \"consults\"]\n",
        "for word in words:\n",
        "  print(word,\"--->\",regexp.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJlTWRdsRsqN",
        "outputId": "48305d06-a00f-49d4-9673-8b0bf01da84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "connecting ---> connect\n",
            "connect ---> connect\n",
            "connects ---> connect\n",
            "fractionally ---> fractionally\n",
            "fractions ---> fraction\n",
            "consult ---> consult\n",
            "consulation ---> consulat\n",
            "consulting ---> consult\n",
            "consults ---> consult\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization using WordnetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "\n",
        "# a denotes adjective in \"pos\"\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
        "\n",
        "# v denotes verb in \"pos\"\n",
        "print(\"took :\", lemmatizer.lemmatize(\"took\", pos=\"v\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFNhtaLzR31S",
        "outputId": "374a70f9-9083-4eba-a1bb-99c23710d180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "better : good\n",
            "took : take\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization using Spacy\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define a sample text\n",
        "text = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract lemmatized tokens\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "# Join the lemmatized tokens into a sentence\n",
        "lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Print the original and lemmatized text\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Lemmatized Text:\", lemmatized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "verBJH0eSDIe",
        "outputId": "47a9db13-1ebd-4a3c-ae94-f8fa161b8b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The quick brown foxes are jumping over the lazy dogs.\n",
            "Lemmatized Text: the quick brown fox be jump over the lazy dog .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SPLIT WORD AND DISPLAY WORD COUNT\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"GeeksforGeeks is a Computer Science platform.\"\n",
        "tokenized_text = word_tokenize(text)\n",
        "print(\"Spilt Words: \", tokenized_text)\n",
        "print(\"Count of Words: \", len(tokenized_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLnb5BzZSJZ-",
        "outputId": "1cdd5178-9b5c-495d-95be-c9d72aefc3e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spilt Words:  ['GeeksforGeeks', 'is', 'a', 'Computer', 'Science', 'platform', '.']\n",
            "Count of Words:  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the para and tokenize, find part of speech of each word (ENGLISH)\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "def tokenize_and_find_pos(paragraph):\n",
        "  sentences = sent_tokenize(paragraph)\n",
        "\n",
        "  for i, sentence in enumerate(sentences, start=1):\n",
        "    print(f\"Sentence {i}: {sentence}\")\n",
        "    words = word_tokenize(sentence)\n",
        "    pos_tags = pos_tag(words)\n",
        "    print(\"Parts of Speech:\", pos_tags, \"\\n\")\n",
        "\n",
        "# Example paragraph\n",
        "paragraph = \"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human (natural) languages. It is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\"\n",
        "\n",
        "tokenize_and_find_pos(paragraph)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMhh-PO3SUZB",
        "outputId": "e6a5df98-ab9e-4fc6-c928-6666f0cb7875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human (natural) languages.\n",
            "Parts of Speech: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('linguistics', 'NNS'), (',', ','), ('computer', 'NN'), ('science', 'NN'), (',', ','), ('and', 'CC'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('interactions', 'NNS'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('(', '('), ('natural', 'JJ'), (')', ')'), ('languages', 'NNS'), ('.', '.')] \n",
            "\n",
            "Sentence 2: It is used to apply algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.\n",
            "Parts of Speech: [('It', 'PRP'), ('is', 'VBZ'), ('used', 'VBN'), ('to', 'TO'), ('apply', 'VB'), ('algorithms', 'JJ'), ('to', 'TO'), ('identify', 'VB'), ('and', 'CC'), ('extract', 'VB'), ('the', 'DT'), ('natural', 'JJ'), ('language', 'NN'), ('rules', 'NNS'), ('such', 'JJ'), ('that', 'IN'), ('the', 'DT'), ('unstructured', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('is', 'VBZ'), ('converted', 'VBN'), ('into', 'IN'), ('a', 'DT'), ('form', 'NN'), ('that', 'IN'), ('computers', 'NNS'), ('can', 'MD'), ('understand', 'VB'), ('.', '.')] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the para and tokenize, find part of speech of each word (NON-ENGLISH)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "german_tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\n",
        "german_tokens=german_tokenizer.tokenize('Wie geht es Ihnen? Gut,danke.')\n",
        "print(german_tokens)\n",
        "ps = pos_tag(german_tokens)\n",
        "print(ps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqmJLhxXSu4V",
        "outputId": "f33b9560-d659-4939-a01a-72918970f070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Wie geht es Ihnen?', 'Gut,danke.']\n",
            "[('Wie geht es Ihnen?', 'NNP'), ('Gut,danke.', 'NNP')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw a parse tree using python for any given sentence in a required grammar rule using the chunk parsing.\n",
        "\n",
        "import nltk\n",
        "from nltk import RegexpParser\n",
        "from nltk.tokenize import word_tokenize\n",
        "#from IPython.display import display\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "# Define a sample grammar rule\n",
        "grammar = r\"\"\"\n",
        "NP: {<DT|JJ|NN.*>+}  # Chunk sequences of DT, JJ, NN\n",
        "PP: {<IN><NP>} # Chunk prepositions followed by NP\n",
        "VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
        "CLAUSE: {<NP><VP>} # Chunk NP, VP pairs\n",
        "\"\"\"\n",
        "# Create a chunk parser\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "# Define a sample sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "# Perform POS tagging\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "# Apply chunk parsing\n",
        "parse_tree = chunk_parser.parse(tagged_tokens)\n",
        "# Display parse tree\n",
        "parse_tree.pretty_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE4WP1vjS42C",
        "outputId": "efd053e6-6b81-4248-89b3-752473218a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    S                                      \n",
            "                                    |                                       \n",
            "                                  CLAUSE                                   \n",
            "           _________________________|_______________                        \n",
            "          |                                         VP                     \n",
            "          |                          _______________|_____                  \n",
            "          |                         |                     PP               \n",
            "          |                         |         ____________|_____            \n",
            "          NP                        |        |                  NP         \n",
            "   _______|________________         |        |       ___________|______     \n",
            "The/DT quick/JJ brown/NN fox/NN jumps/VBZ over/IN the/DT     lazy/JJ dog/NN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a python code to find the term frequency and inverse document frequency for three documents. (Consider 3 documents as 3 paragraphs)\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "def calculate_tf(text):\n",
        "  words = text.lower().split()\n",
        "  word_count = len(words)\n",
        "  word_freq = Counter(words)\n",
        "  tf = {word: freq / word_count for word, freq in word_freq.items()}\n",
        "  return tf\n",
        "def calculate_idf(documents):\n",
        "  total_docs = len(documents)\n",
        "  idf = {}\n",
        "  all_words = [word for document in documents for word in\n",
        "  set(document.lower().split())]\n",
        "  for word in all_words:\n",
        "    doc_count = sum([1 for document in documents if word in\n",
        "    document.lower().split()])\n",
        "    idf[word] = math.log(total_docs / (1 + doc_count))\n",
        "  return idf\n",
        "# Example documents\n",
        "document1 = \"This is the first document. It contains words to analyze term frequency and inverse document frequency.\"\n",
        "document2 = \"The second document has some overlapping words with the first document but also includes unique terms.\"\n",
        "document3 = \"Finally, the third document is shorter and has fewer words compared to the other two documents.\"\n",
        "documents = [document1, document2, document3]\n",
        "# Calculate TF for each document\n",
        "tf_documents = [calculate_tf(document) for document in documents]\n",
        "# Calculate IDF for all documents\n",
        "idf = calculate_idf(documents)\n",
        "print(\"Term Frequency (TF) for each document:\")\n",
        "for i, tf_doc in enumerate(tf_documents, start=1):\n",
        "  print(f\"Document {i}: {tf_doc}\")\n",
        "  print(\"\\nInverse Document Frequency (IDF) for all words:\")\n",
        "for word, idf_value in idf.items():\n",
        "  print(f\"{word}: {idf_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p147-kfad0v-",
        "outputId": "0a7ed8e5-02f4-49f5-fa18-1c9841e6c40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term Frequency (TF) for each document:\n",
            "Document 1: {'this': 0.0625, 'is': 0.0625, 'the': 0.0625, 'first': 0.0625, 'document.': 0.0625, 'it': 0.0625, 'contains': 0.0625, 'words': 0.0625, 'to': 0.0625, 'analyze': 0.0625, 'term': 0.0625, 'frequency': 0.0625, 'and': 0.0625, 'inverse': 0.0625, 'document': 0.0625, 'frequency.': 0.0625}\n",
            "\n",
            "Inverse Document Frequency (IDF) for all words:\n",
            "Document 2: {'the': 0.125, 'second': 0.0625, 'document': 0.125, 'has': 0.0625, 'some': 0.0625, 'overlapping': 0.0625, 'words': 0.0625, 'with': 0.0625, 'first': 0.0625, 'but': 0.0625, 'also': 0.0625, 'includes': 0.0625, 'unique': 0.0625, 'terms.': 0.0625}\n",
            "\n",
            "Inverse Document Frequency (IDF) for all words:\n",
            "Document 3: {'finally,': 0.0625, 'the': 0.125, 'third': 0.0625, 'document': 0.0625, 'is': 0.0625, 'shorter': 0.0625, 'and': 0.0625, 'has': 0.0625, 'fewer': 0.0625, 'words': 0.0625, 'compared': 0.0625, 'to': 0.0625, 'other': 0.0625, 'two': 0.0625, 'documents.': 0.0625}\n",
            "\n",
            "Inverse Document Frequency (IDF) for all words:\n",
            "term: 0.4054651081081644\n",
            "frequency: 0.4054651081081644\n",
            "analyze: 0.4054651081081644\n",
            "words: -0.2876820724517809\n",
            "contains: 0.4054651081081644\n",
            "it: 0.4054651081081644\n",
            "the: -0.2876820724517809\n",
            "to: 0.0\n",
            "and: 0.0\n",
            "first: 0.0\n",
            "document: -0.2876820724517809\n",
            "is: 0.0\n",
            "this: 0.4054651081081644\n",
            "frequency.: 0.4054651081081644\n",
            "document.: 0.4054651081081644\n",
            "inverse: 0.4054651081081644\n",
            "includes: 0.4054651081081644\n",
            "overlapping: 0.4054651081081644\n",
            "also: 0.4054651081081644\n",
            "second: 0.4054651081081644\n",
            "but: 0.4054651081081644\n",
            "some: 0.4054651081081644\n",
            "has: 0.0\n",
            "with: 0.4054651081081644\n",
            "unique: 0.4054651081081644\n",
            "terms.: 0.4054651081081644\n",
            "compared: 0.4054651081081644\n",
            "documents.: 0.4054651081081644\n",
            "finally,: 0.4054651081081644\n",
            "two: 0.4054651081081644\n",
            "fewer: 0.4054651081081644\n",
            "shorter: 0.4054651081081644\n",
            "third: 0.4054651081081644\n",
            "other: 0.4054651081081644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement a python code to remove Stop words and identify Parts of Speech for a given paragraph.\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag\n",
        "def remove_stopwords(text):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "  return ' '.join(filtered_text)\n",
        "def identify_pos(text):\n",
        "  sentences = sent_tokenize(text)\n",
        "  tagged_sentences = [pos_tag(word_tokenize(sentence)) for sentence in sentences]\n",
        "  return tagged_sentences\n",
        "# Example paragraph\n",
        "paragraph = \"\"\"\n",
        "Natural Language Processing (NLP) is a subfield of artificial\n",
        "intelligence concerned with the interaction between computers and\n",
        "humans in natural language. It focuses on the interaction between\n",
        "computers and humans in the natural language and it is a field at the\n",
        "intersection of computer science, artificial intelligence, and\n",
        "computational linguistics.\n",
        "\"\"\"\n",
        "print(\"paragraph: \", paragraph)\n",
        "# Remove stop words\n",
        "paragraph_without_stopwords = remove_stopwords(paragraph)\n",
        "print(\"Paragraph without stopwords:\")\n",
        "print(paragraph_without_stopwords)\n",
        "# Identify Parts of Speech\n",
        "tagged_sentences = identify_pos(paragraph)\n",
        "print(\"\\nParts of speech:\")\n",
        "for sentence in tagged_sentences:\n",
        "  print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y9WimqReZ12",
        "outputId": "b345be7c-e432-48e1-9aa6-46e4d54ed18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paragraph:  \n",
            "Natural Language Processing (NLP) is a subfield of artificial\n",
            "intelligence concerned with the interaction between computers and\n",
            "humans in natural language. It focuses on the interaction between\n",
            "computers and humans in the natural language and it is a field at the\n",
            "intersection of computer science, artificial intelligence, and\n",
            "computational linguistics.\n",
            "\n",
            "Paragraph without stopwords:\n",
            "Natural Language Processing ( NLP ) subfield artificial intelligence concerned interaction computers humans natural language . focuses interaction computers humans natural language field intersection computer science , artificial intelligence , computational linguistics .\n",
            "\n",
            "Parts of speech:\n",
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('humans', 'NNS'), ('in', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('focuses', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('humans', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('natural', 'JJ'), ('language', 'NN'), ('and', 'CC'), ('it', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('at', 'IN'), ('the', 'DT'), ('intersection', 'NN'), ('of', 'IN'), ('computer', 'NN'), ('science', 'NN'), (',', ','), ('artificial', 'JJ'), ('intelligence', 'NN'), (',', ','), ('and', 'CC'), ('computational', 'JJ'), ('linguistics', 'NNS'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given two words, calculate the similarity between the words\n",
        "# a.\tBy using Path Similarity\n",
        "# b.\tBy using Wu-Palmer Similarity\n",
        "\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def calculate_similarities(word1, word2):\n",
        "  synsets1 = wn.synsets(word1)\n",
        "  synsets2 = wn.synsets(word2)\n",
        "\n",
        "  if not synsets1 or not synsets2:\n",
        "    print(f\"No synsets found for one of the words: {word1}, {word2}\")\n",
        "    return\n",
        "\n",
        "  synset1 = synsets1[0]\n",
        "  synset2 = synsets2[0]\n",
        "\n",
        "  path_similarity = synset1.path_similarity(synset2)\n",
        "\n",
        "  wup_similarity = synset1.wup_similarity(synset2)\n",
        "\n",
        "  print(f\"Path Similarity between '{word1}' and '{word2}':\", path_similarity)\n",
        "  print(f\"Wu-Palmer Similarity between '{word1}' and '{word2}':\", wup_similarity)\n",
        "\n",
        "word1 = \"consultant\"\n",
        "word2 = \"consultancy\"\n",
        "\n",
        "calculate_similarities(word1, word2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFMNmVuBei66",
        "outputId": "736c6b78-0533-418d-8d0f-dd8059357ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path Similarity between 'consultant' and 'consultancy': 0.06666666666666667\n",
            "Wu-Palmer Similarity between 'consultant' and 'consultancy': 0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider a sentence and do the following\n",
        "# a.\tImport the libraries.\n",
        "# b.\tThen apply word tokenization and part-of-speech tagging to the sentence.\n",
        "# c.\tCreate a chunk parser and test it on the sentence.\n",
        "# d.\tIdentify nationalities or religious or political groups, organization, date and money in the given sentence.(select sentence appropriately)\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from nltk.chunk.regexp import RegexpParser\n",
        "from nltk import ne_chunk\n",
        "\n",
        "sentence = \"In August, India and Microsoft plan to address the issue of climate change and alloted $5000000 for it.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "chunk_pattern = r\"NP: {<DT>?<JJ>*<NN>}\"\n",
        "cp = RegexpParser(chunk_pattern)\n",
        "chunked = cp.parse(tagged)\n",
        "ne_chunked = ne_chunk(tagged)\n",
        "\n",
        "print(\"POS Tags:\", tagged)\n",
        "chunked.pretty_print()\n",
        "\n",
        "iob_tagged = tree2conlltags(ne_chunked)\n",
        "print(\"IOB Tags:\", iob_tagged)\n",
        "\n",
        "entities = ['GPE', 'ORGANIZATION', 'DATE', 'MONEY','COUNTRY']\n",
        "for token, pos, entity in iob_tagged:\n",
        "  if entity in entities:\n",
        "    print(f\"{entity}: {token}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqzujWYlfIl1",
        "outputId": "0810b1f0-9ddc-4614-d3f3-78e8944d2c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('In', 'IN'), ('August', 'NNP'), (',', ','), ('India', 'NNP'), ('and', 'CC'), ('Microsoft', 'NNP'), ('plan', 'NN'), ('to', 'TO'), ('address', 'VB'), ('the', 'DT'), ('issue', 'NN'), ('of', 'IN'), ('climate', 'NN'), ('change', 'NN'), ('and', 'CC'), ('alloted', 'VBD'), ('$', '$'), ('5000000', 'CD'), ('for', 'IN'), ('it', 'PRP'), ('.', '.')]\n",
            "                                                                                       S                                                                                       \n",
            "   ____________________________________________________________________________________|__________________________________________________________________________________      \n",
            "  |       |       |      |       |          |         |       |        |     |         |       |      |        |      |     |     NP           NP              NP         NP   \n",
            "  |       |       |      |       |          |         |       |        |     |         |       |      |        |      |     |     |       _____|_____          |          |     \n",
            "In/IN August/NNP ,/, India/NNP and/CC Microsoft/NNP to/TO address/VB of/IN and/CC alloted/VBD $/$ 5000000/CD for/IN it/PRP ./. plan/NN the/DT     issue/NN climate/NN change/NN\n",
            "\n",
            "IOB Tags: [('In', 'IN', 'O'), ('August', 'NNP', 'B-GPE'), (',', ',', 'O'), ('India', 'NNP', 'B-GPE'), ('and', 'CC', 'O'), ('Microsoft', 'NNP', 'B-ORGANIZATION'), ('plan', 'NN', 'O'), ('to', 'TO', 'O'), ('address', 'VB', 'O'), ('the', 'DT', 'O'), ('issue', 'NN', 'O'), ('of', 'IN', 'O'), ('climate', 'NN', 'O'), ('change', 'NN', 'O'), ('and', 'CC', 'O'), ('alloted', 'VBD', 'O'), ('$', '$', 'O'), ('5000000', 'CD', 'O'), ('for', 'IN', 'O'), ('it', 'PRP', 'O'), ('.', '.', 'O')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write down the syntax for the following:\n",
        "# a.\tImport word net, Use the term \"hello\" to find Synsets\n",
        "# b.\tUsing Synset find the element in the 0th index, Just the word (using lemmas)\n",
        "# c.\tName, Definition of that first (0th index) Synset and examples of the word.\n",
        "# d.\tDiscern synonyms and antonyms in Synset\n",
        "# e.\tDiscern Hypernyms and Hyponyms in Synset\n",
        "\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "hello_synsets = wn.synsets('hello')\n",
        "print(\"All synsets for 'hello':\", hello_synsets)\n",
        "\n",
        "first_synset = hello_synsets[0]\n",
        "print(\"\\nFirst Synset:\", first_synset)\n",
        "\n",
        "first_lemma = first_synset.lemmas()[0].name()\n",
        "print(\"First lemma name of the 0th Synset:\", first_lemma)\n",
        "\n",
        "synset_name = first_synset.name()\n",
        "synset_definition = first_synset.definition()\n",
        "synset_examples = first_synset.examples()\n",
        "print(\"\\nName of the 0th Synset:\", synset_name)\n",
        "print(\"Definition of the 0th Synset:\", synset_definition)\n",
        "print(\"Examples of the 0th Synset:\", synset_examples)\n",
        "\n",
        "synonyms = [lemma.name() for lemma in first_synset.lemmas()]\n",
        "antonyms = [ant.name() for lemma in first_synset.lemmas() for ant in lemma.antonyms()]\n",
        "print(\"\\nSynonyms in Synset:\", synonyms)\n",
        "print(\"Antonyms in Synset:\", antonyms)\n",
        "\n",
        "hypernyms = first_synset.hypernyms()\n",
        "hyponyms = first_synset.hyponyms()\n",
        "print(\"\\nHypernyms of the 0th Synset:\", hypernyms)\n",
        "print(\"Hyponyms of the 0th Synset:\", hyponyms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmiluzNXfTOE",
        "outputId": "56dfa1ab-d365-4643-a13a-a0ca571b072c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All synsets for 'hello': [Synset('hello.n.01')]\n",
            "\n",
            "First Synset: Synset('hello.n.01')\n",
            "First lemma name of the 0th Synset: hello\n",
            "\n",
            "Name of the 0th Synset: hello.n.01\n",
            "Definition of the 0th Synset: an expression of greeting\n",
            "Examples of the 0th Synset: ['every morning they exchanged polite hellos']\n",
            "\n",
            "Synonyms in Synset: ['hello', 'hullo', 'hi', 'howdy', 'how-do-you-do']\n",
            "Antonyms in Synset: []\n",
            "\n",
            "Hypernyms of the 0th Synset: [Synset('greeting.n.01')]\n",
            "Hyponyms of the 0th Synset: []\n"
          ]
        }
      ]
    }
  ]
}